# LLMs from Scratch

This repository is a hands-on exploration of building **Large Language Models (LLMs) from scratch** using Python and PyTorch. The goal is to understand the fundamentals of language modeling, tokenization, embeddings, and transformer architectures through practical coding exercises.

## Repository Structure

```

llms-from-scratch/
├── data/               # Raw and processed text datasets
│   ├── raw/
│   └── processed/
├── notebooks/          # Jupyter notebooks with step-by-step experiments
│   ├── 01_neural_network_basics.ipynb
│   ├── 02_tokenization.ipynb
│   └── 03_embeddings_and_attention.ipynb
├── README.md           # This file
├── requirements.txt    # Python dependencies
```

## Notebooks

- **01_neural_network_basics.ipynb** - Build a simple feedforward neural network for text data.
- **02_tokenization.ipynb** - Implement tokenization techniques including byte-pair encoding (BPE).
- **03_embeddings_and_attention.ipynb** - Explore embeddings, attention mechanisms, and transformer basics.

## Getting Started

1. Clone the repository:

```bash
git clone https://github.com/your-username/llms-from-scratch.git
cd llms-from-scratch
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Open notebooks in `notebooks/` to explore step-by-step.

## Contributing

Feel free to fork, experiment, and create pull requests. Open discussions for ideas and improvements are welcome.
